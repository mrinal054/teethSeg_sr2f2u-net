{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd1n8fUaxGbt"
      },
      "source": [
        "# S-R2F2U-Net: A single-stage model for teeth segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation is leveraged from [Yingkai Shaâ€™s](https://github.com/yingkaisha/keras-unet-collection) repository. Base models for recurrent, residual and attention are taken from the above mentioned repository. "
      ],
      "metadata": {
        "id": "WrdBKGz_NaBJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK8GtPTRVdEZ",
        "outputId": "3845689c-6035-420d-94c6-d2e1cdbdd07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive  \n",
        "drive.mount('/content/drive')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWFMrBMKVnJ8"
      },
      "outputs": [],
      "source": [
        "# Set the system path for saving and loading libraries\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/library')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19AWOkr7c-7J"
      },
      "outputs": [],
      "source": [
        "# Uncomment to install keras unet collection\n",
        "# !pip install --target='/content/drive/MyDrive/library' keras-unet-collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-brKwKddeWA",
        "outputId": "30242906-9e7c-43a1-e527-962be5d83b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n",
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Check tensorflow and keras versions\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "print(tf.__version__)\n",
        "print(keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDEjrVEndeLC",
        "outputId": "33d52bc0-9e91-48d1-c50a-2659786c0df6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "#Make sure the GPU is available. \n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcSbEgAhen3r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from datetime import datetime \n",
        "import cv2\n",
        "import json\n",
        "from PIL import Image\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TerminateOnNaN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9goo7m--fzvN"
      },
      "outputs": [],
      "source": [
        "# Load directories\n",
        "img_dir_train = '/content/drive/MyDrive/panoramicDentalSegmentation/patch_512/train/images'\n",
        "img_dir_val = '/content/drive/MyDrive/panoramicDentalSegmentation/patch_512/val/images'\n",
        "mask_dir_train = '/content/drive/MyDrive/panoramicDentalSegmentation/patch_512/train/mask'\n",
        "mask_dir_val = '/content/drive/MyDrive/panoramicDentalSegmentation/patch_512/val/mask'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM4TqOhg9aEA"
      },
      "outputs": [],
      "source": [
        "from keras_unet_collection import models, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXwM5vRrN2lG"
      },
      "source": [
        "### Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGfAIOb0N1kv"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow.compat.v1 import image\n",
        "from tensorflow.keras.layers import MaxPooling2D, MaxPooling3D, AveragePooling2D, AveragePooling3D, UpSampling2D, UpSampling3D, Conv2DTranspose, Conv3DTranspose, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Conv2D, Conv3D, Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, concatenate, multiply, add\n",
        "from tensorflow.keras.layers import ReLU, LeakyReLU, PReLU, ELU, Softmax\n",
        "\n",
        "def decode_layer(X, channel, conv, pool_size, unpool, kernel_size=3, \n",
        "                 activation='ReLU', batch_norm=False, name='decode'):\n",
        "    '''\n",
        "    An overall decode layer, based on either upsampling or trans conv.\n",
        "    \n",
        "    decode_layer(X, channel, conv, pool_size, unpool, kernel_size=3,\n",
        "                 activation='ReLU', batch_norm=False, name='decode')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        pool_size: the decoding factor.\n",
        "        channel: (for trans conv only) number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.           \n",
        "        kernel_size: size of convolution kernels. \n",
        "                     If kernel_size='auto', then it equals to the `pool_size`.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    * The defaut: `kernel_size=3`, is suitable for `pool_size=2`.\n",
        "    \n",
        "    '''\n",
        "    # Set 2d or 3d convolution\n",
        "    if conv == '2d': \n",
        "      conv_nd = Conv2D\n",
        "      ConvTranspose_nd = Conv2DTranspose\n",
        "    elif conv == '3d': \n",
        "      conv_nd = Conv3D\n",
        "      ConvTranspose_nd = Conv3DTranspose\n",
        "    else: raise ValueError('Wrong keyword for conv')\n",
        "\n",
        "    # parsers\n",
        "    if unpool is False:\n",
        "        # trans conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    elif unpool == 'nearest':\n",
        "        # upsample2d configurations\n",
        "        unpool = True\n",
        "        interp = 'nearest'\n",
        "    \n",
        "    elif (unpool is True) or (unpool == 'bilinear'):\n",
        "        # upsample2d configurations\n",
        "        unpool = True\n",
        "        interp = 'bilinear'\n",
        "    \n",
        "    else:\n",
        "        raise ValueError('Invalid unpool keyword')\n",
        "        \n",
        "    if unpool:\n",
        "        if conv == '2d':\n",
        "          X = UpSampling2D(size=pool_size, interpolation=interp, name='{}_unpool'.format(name))(X)\n",
        "        elif conv == '3d':\n",
        "          X = UpSampling2D(size=pool_size, interpolation=interp, name='{}_unpool'.format(name))(X)\n",
        "        else:\n",
        "          raise ValueError('Wrong keyword for conv')\n",
        "\n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "            \n",
        "        X = ConvTranspose_nd(channel, kernel_size, strides=pool_size, \n",
        "                            padding='same', name='{}_trans_conv'.format(name))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=-1, name='{}_bn'.format(name))(X)     # axis changed\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "        \n",
        "    return X\n",
        "\n",
        "def encode_layer(X, channel, conv, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode'):\n",
        "    '''\n",
        "    An overall encode layer, based on one of the:\n",
        "    (1) max-pooling, (2) average-pooling, (3) strided conv2d.\n",
        "    \n",
        "    encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        pool_size: the encoding factor.\n",
        "        channel: (for strided conv only) number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        kernel_size: size of convolution kernels. \n",
        "                     If kernel_size='auto', then it equals to the `pool_size`.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    # Set 2d or 3d convolution\n",
        "    if conv == '2d': \n",
        "      conv_nd = Conv2D\n",
        "      MaxPooling_nd = MaxPooling2D\n",
        "      AveragePooling_nd = AveragePooling2D\n",
        "    elif conv == '3d': \n",
        "      conv_nd = Conv3D\n",
        "      MaxPooling_nd = MaxPooling2D\n",
        "      AveragePooling_nd = AveragePooling2D      \n",
        "    else: raise ValueError('Wrong keyword for conv or max-pool or avg-pool')\n",
        "\n",
        "    # parsers\n",
        "    if (pool in [False, True, 'max', 'ave']) is not True:\n",
        "        raise ValueError('Invalid pool keyword')\n",
        "        \n",
        "    # maxpooling2d as default\n",
        "    if pool is True:\n",
        "        pool = 'max'\n",
        "        \n",
        "    elif pool is False:\n",
        "        # stride conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    if pool == 'max':\n",
        "        X = MaxPooling_nd(pool_size=pool_size, name='{}_maxpool'.format(name))(X)\n",
        "        \n",
        "    elif pool == 'ave':\n",
        "        X = AveragePooling_nd(pool_size=pool_size, name='{}_avepool'.format(name))(X)\n",
        "        \n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "        \n",
        "        # linear convolution with strides\n",
        "        X = conv_nd(channel, kernel_size, strides=pool_size, padding='valid', use_bias=bias_flag, name='{}_stride_conv'.format(name))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=-1, name='{}_bn'.format(name))(X)  # changed axis\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "            \n",
        "    return X\n",
        "\n",
        "def attention_gate(X, g, channel, conv,  \n",
        "                   activation='ReLU', \n",
        "                   attention='add', name='att'):\n",
        "    '''\n",
        "    Self-attention gate modified from Oktay et al. 2018.\n",
        "    \n",
        "    attention_gate(X, g, channel,  activation='ReLU', attention='add', name='att')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor, i.e., key and value.\n",
        "        g: gated tensor, i.e., query.\n",
        "        channel: number of intermediate channel.\n",
        "                 Oktay et al. (2018) did not specify (denoted as F_int).\n",
        "                 intermediate channel is expected to be smaller than the input channel.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        activation: a nonlinear attnetion activation.\n",
        "                    The `sigma_1` in Oktay et al. 2018. Default is 'ReLU'.\n",
        "        attention: 'add' for additive attention; 'multiply' for multiplicative attention.\n",
        "                   Oktay et al. 2018 applied additive attention.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X_att: output tensor.\n",
        "    \n",
        "    '''\n",
        "    # Set 2d or 3d convolution\n",
        "    if conv == '2d': conv_nd = Conv2D\n",
        "    elif conv == '3d': conv_nd = Conv3D\n",
        "    else: raise ValueError('Wrong keyword for conv')\n",
        "\n",
        "    activation_func = eval(activation)\n",
        "    attention_func = eval(attention)\n",
        "    \n",
        "    # mapping the input tensor to the intermediate channel\n",
        "    theta_att = conv_nd(channel, 1, use_bias=True, name='{}_theta_x'.format(name))(X)\n",
        "    \n",
        "    # mapping the gate tensor\n",
        "    phi_g = conv_nd(channel, 1, use_bias=True, name='{}_phi_g'.format(name))(g)\n",
        "    \n",
        "    # ----- attention learning ----- #\n",
        "    query = attention_func([theta_att, phi_g], name='{}_add'.format(name))\n",
        "    \n",
        "    # nonlinear activation\n",
        "    f = activation_func(name='{}_activation'.format(name))(query)\n",
        "    \n",
        "    # linear transformation\n",
        "    psi_f = conv_nd(1, 1, use_bias=True, name='{}_psi_f'.format(name))(f)\n",
        "    # ------------------------------ #\n",
        "    \n",
        "    # sigmoid activation as attention coefficients\n",
        "    coef_att = Activation('sigmoid', name='{}_sigmoid'.format(name))(psi_f)\n",
        "    \n",
        "    # multiplicative attention masking\n",
        "    X_att = multiply([X, coef_att], name='{}_masking'.format(name))\n",
        "    \n",
        "    return X_att\n",
        "\n",
        "def CONV_stack(X, channel, conv, kernel_size=3, stack_num=2, \n",
        "               dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack'):\n",
        "    '''\n",
        "    Stacked convolutional layers:\n",
        "    (Convolutional layer --> batch normalization --> Activation)*stack_num\n",
        "    \n",
        "    CONV_stack(X, channel, kernel_size=3, stack_num=2, dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack')\n",
        "    \n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked Conv2D-BN-Activation layers.\n",
        "        dilation_rate: optional dilated convolution kernel.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor\n",
        "        \n",
        "    '''\n",
        "    # Set 2d or 3d convolution\n",
        "    if conv == '2d': conv_nd = Conv2D\n",
        "    elif conv == '3d': conv_nd = Conv3D\n",
        "    else: raise ValueError('Wrong keyword for conv')\n",
        "\n",
        "    bias_flag = not batch_norm\n",
        "    \n",
        "    # stacking Convolutional layers\n",
        "    for i in range(stack_num):\n",
        "        \n",
        "        activation_func = eval(activation)\n",
        "        \n",
        "        # linear convolution\n",
        "        X = conv_nd(channel, kernel_size, padding='same', use_bias=bias_flag, \n",
        "                   dilation_rate=dilation_rate, name='{}_{}'.format(name, i))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=-1, name='{}_{}_bn'.format(name, i))(X)       # axis changed\n",
        "        \n",
        "        # activation\n",
        "        activation_func = eval(activation)\n",
        "        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\n",
        "        \n",
        "    return X\n",
        "\n",
        "def Res_CONV_stack(X, X_skip, channel, conv, res_num, activation='ReLU', batch_norm=False, name='res_conv'):\n",
        "    '''\n",
        "    Stacked convolutional layers with residual path.\n",
        "     \n",
        "    Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv')\n",
        "     \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        X_skip: the tensor that does go into the residual path \n",
        "                can be a copy of X (e.g., the identity block of ResNet).\n",
        "        channel: number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d' \n",
        "        res_num: number of convolutional layers within the residual path.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''  \n",
        "    X = CONV_stack(X, channel, conv=conv, kernel_size=3, stack_num=res_num, dilation_rate=1, \n",
        "                   activation=activation, batch_norm=batch_norm, name=name)\n",
        "\n",
        "    X = add([X_skip, X], name='{}_add'.format(name))\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    X = activation_func(name='{}_add_activation'.format(name))(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def CONV_output(X, conv, n_labels, kernel_size=1, activation='Softmax', name='conv_output'):\n",
        "    '''\n",
        "    Convolutional layer with output activation.\n",
        "    \n",
        "    CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        n_labels: number of classification label(s).\n",
        "        kernel_size: size of 2-d convolution kernels. Default is 1-by-1.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                    Default option is 'Softmax'.\n",
        "                    if None is received, then linear activation is applied.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    # Set 2d or 3d convolution\n",
        "    if conv == '2d': conv_nd = Conv2D\n",
        "    elif conv == '3d': conv_nd = Conv3D\n",
        "    else: raise ValueError('Wrong keyword for conv')\n",
        "\n",
        "    X = conv_nd(n_labels, kernel_size, padding='same', use_bias=True, name=name)(X)\n",
        "    \n",
        "    if activation:\n",
        "        \n",
        "        if activation == 'Sigmoid':\n",
        "            X = Activation('sigmoid', name='{}_activation'.format(name))(X)\n",
        "            \n",
        "        else:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "            \n",
        "    return X\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjKdW8ixN__w"
      },
      "source": [
        "### Hybrid U-Net (Combines recurrent, residual, and attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQGqX32EubFm"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def any_recur(val) : \n",
        "  \"\"\"\n",
        "  Checks if it is a basic U-Net or Recurrent U-Net\n",
        "  \n",
        "  Input\n",
        "  -----------\n",
        "  val: A tuple or list. e.g. val=(True, False)\n",
        "\n",
        "  Output\n",
        "  -----------\n",
        "  result: Boolean type. 'True' for recurrent U-Net, 'False' for basic U-Net\n",
        "  \n",
        "  \"\"\"\n",
        "  result = False \n",
        "  for ele in val: result += ele \n",
        "  if result > 0: result = True\n",
        "  else: result = False\n",
        "\n",
        "  return result  \n",
        "\n",
        "def RR_CONV(X, channel, conv, kernel_size=3, stack_num=2,  \n",
        "            dilation_rate=1, \n",
        "            filter_double=False, \n",
        "            recur_status=(True, True), recur_num=2, \n",
        "            is_residual=True, \n",
        "            activation='ReLU', batch_norm=False, name='rr'):\n",
        "    '''\n",
        "    Recurrent convolutional layers with skip connection.\n",
        "    \n",
        "    RR_CONV(X, channel, conv, kernel_size=3, stack_num=2, recur_num=2, activation='ReLU', batch_norm=False, name='rr')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked recurrent convolutional layers.\n",
        "        recur_num: number of recurrent iterations.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    assert len(recur_status) == stack_num, \"Length of recur_status should be equal to stack_num\"\n",
        "    \n",
        "    activation_func = eval(activation)    \n",
        "    \n",
        "    print('Recurrent Status: ', recur_status)\n",
        "    print('Recurrent: ', any_recur(recur_status))\n",
        "    print('Residual: ', is_residual)\n",
        "    print('Filter double: ', filter_double)\n",
        "\n",
        "    # Set 2d or 3d convolution\n",
        "    if conv == '2d': conv_nd = Conv2D\n",
        "    elif conv == '3d': conv_nd = Conv3D\n",
        "    else: raise ValueError('Wrong keyword for conv')\n",
        "\n",
        "    if filter_double: \n",
        "      \"\"\"\n",
        "      Let's say input chanel=3, base filter (or channel)=32, stack_num=3, and filter_double=True. \n",
        "      Then filter numbers will be 3 -> 32 -> 64-> 128. Filters will be doubled in each stack.\n",
        "      In this case, skip-connection layer should have 128 filters, otherwise we can't add them.\n",
        "      So, the filter number of skip-connection layer = base filter x 2^(stack_num-1) = 32 x 2^(3-1) = 128\n",
        "      \"\"\"\n",
        "      layer_skip = conv_nd(channel*(2**(stack_num-1)), 1, dilation_rate=dilation_rate, name='{}_layer_skip'.format(name))(X)\n",
        "\n",
        "      \"\"\" \n",
        "      Check if it is a basic U-Net or Recurrent U-Net.\n",
        "      any_recur=False means there is no recurrent operation. So, it is a simple U-Net. In this case, layer_main=X. \n",
        "      \"\"\"\n",
        "      if any_recur(recur_status): layer_main = conv_nd(channel, 1, dilation_rate=dilation_rate, name='{}_layer_main'.format(name))(X)  # Recurrent U-Net       \n",
        "      else: layer_main = X  # Basic U-Net    \n",
        "        \n",
        "    else: # no filter doubling\n",
        "      layer_skip = conv_nd(channel, 1, dilation_rate=dilation_rate, name='{}_conv'.format(name))(X)\n",
        "      if any_recur(recur_status): layer_main = layer_skip        \n",
        "      else: layer_main = X     \n",
        "    \n",
        "    for i in range(stack_num):\n",
        "\n",
        "      if i>0 and recur_status[i] and filter_double: \n",
        "        layer_main = conv_nd(channel, 1, dilation_rate=dilation_rate, name='{}_conv'.format(name))(layer_main)\n",
        "\n",
        "      layer_res = conv_nd(channel, kernel_size, padding='same', dilation_rate=dilation_rate, name='{}_conv{}'.format(name, i))(layer_main)\n",
        "      \n",
        "      if batch_norm:\n",
        "          layer_res = BatchNormalization(name='{}_bn{}'.format(name, i))(layer_res)\n",
        "          \n",
        "      layer_res = activation_func(name='{}_activation{}'.format(name, i))(layer_res)\n",
        "\n",
        "      # Recurrent\n",
        "      if recur_status[i]:\n",
        "          \n",
        "        for j in range(recur_num):\n",
        "            \n",
        "            layer_add = add([layer_res, layer_main], name='{}_add{}_{}'.format(name, i, j))\n",
        "            \n",
        "            layer_res = conv_nd(channel, kernel_size, padding='same', dilation_rate=dilation_rate, name='{}_conv{}_{}'.format(name, i, j))(layer_add)\n",
        "            \n",
        "            if batch_norm:\n",
        "                layer_res = BatchNormalization(name='{}_bn{}_{}'.format(name, i, j))(layer_res)\n",
        "                \n",
        "            layer_res = activation_func(name='{}_activation{}_{}'.format(name, i, j))(layer_res)\n",
        "          \n",
        "      layer_main = layer_res\n",
        "\n",
        "      if filter_double: channel = channel * 2 # doubling filter numbers\n",
        "\n",
        "    # Residual\n",
        "    if is_residual: out_layer = add([layer_main, layer_skip], name='{}_add{}'.format(name, i)) # for residual connection\n",
        "    else: out_layer = layer_main\n",
        "\n",
        "    return out_layer\n",
        "\n",
        "\n",
        "def UNET_RR_left(X, channel, conv, kernel_size=3, \n",
        "                 stack_num=2, \n",
        "                 dilation_rate=1, \n",
        "                 filter_double=False, \n",
        "                 recur_status=(True, True), recur_num=2, \n",
        "                 is_residual=True, \n",
        "                 is_attention=False, \n",
        "                 atten_activation='ReLU', attention='add',\n",
        "                 activation='ReLU', output_activation='Softmax',\n",
        "                 pool=True, batch_norm=False, name='left0'):\n",
        "    '''\n",
        "    The encoder block of R2U-Net.\n",
        "    \n",
        "    UNET_RR_left(X, channel, conv, kernel_size=3, \n",
        "                 stack_num=2, recur_num=2, activation='ReLU', \n",
        "                 pool=True, batch_norm=False, name='left0')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked recurrent convolutional layers.\n",
        "        recur_num: number of recurrent iterations.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    *downsampling is fixed to 2-by-2, e.g., reducing feature map sizes from 64-by-64 to 32-by-32\n",
        "    '''\n",
        "    pool_size = 2\n",
        "    \n",
        "    # maxpooling layer vs strided convolutional layers\n",
        "    X = encode_layer(X, channel, conv, pool_size, pool, activation=activation, \n",
        "                     batch_norm=batch_norm, name='{}_encode'.format(name))\n",
        "\n",
        "    # stack linear convolutional layers\n",
        "    X = RR_CONV(X, channel, conv=conv, stack_num=stack_num, \n",
        "                dilation_rate=dilation_rate, filter_double=filter_double,\n",
        "                recur_status=recur_status, recur_num=recur_num, \n",
        "                is_residual=is_residual,\n",
        "                activation=activation, batch_norm=batch_norm, name=name)    \n",
        "    return X\n",
        "\n",
        "\n",
        "def UNET_RR_right(X, X_list, channel, conv, kernel_size=3, \n",
        "                  stack_num=2, \n",
        "                  dilation_rate=1, \n",
        "                  filter_double=False, \n",
        "                  recur_status=(True, True), recur_num=2, \n",
        "                  is_residual=True, \n",
        "                  is_attention=False, \n",
        "                  atten_activation='ReLU', attention='add', \n",
        "                  activation='ReLU',\n",
        "                  unpool=True, batch_norm=False, name='right0'):\n",
        "    '''\n",
        "    The decoder block of R2U-Net.\n",
        "    \n",
        "    UNET_RR_right(X, X_list, channel, conv, kernel_size=3, \n",
        "                  stack_num=2, recur_num=2, activation='ReLU',\n",
        "                  unpool=True, batch_norm=False, name='right0')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        X_list: a list of other tensors that connected to the input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked recurrent convolutional layers.\n",
        "        recur_num: number of recurrent iterations.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    pool_size = 2\n",
        "    \n",
        "    X = decode_layer(X, channel, conv, pool_size, unpool, \n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n",
        "    \n",
        "    # linear convolutional layers before concatenation (*********)\n",
        "    X = CONV_stack(X, channel, conv, kernel_size, stack_num=1, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv_before_concat'.format(name))\n",
        "    \n",
        "    # Attention gate\n",
        "    print('Attention: ', is_attention)\n",
        "    if is_attention:\n",
        "      X_left = attention_gate(X=X_list, g=X, channel=channel//2, conv=conv, activation=atten_activation, \n",
        "                            attention=attention, name='{}_att'.format(name)) ################### activation and attention manual removed\n",
        "      # Tensor concatenation\n",
        "      H = concatenate([X, X_left], axis=-1, name='{}_att_concat'.format(name))\n",
        "    else:\n",
        "      # Tensor concatenation\n",
        "      H = concatenate([X, X_list], axis=-1, name='{}_concat'.format(name))\n",
        "    # stacked linear convolutional layers after concatenation\n",
        "    H = RR_CONV(H, channel, conv, stack_num=stack_num, \n",
        "                dilation_rate=dilation_rate, filter_double=filter_double,\n",
        "                recur_status=recur_status, recur_num=recur_num, \n",
        "                is_residual=is_residual,\n",
        "                activation=activation, batch_norm=batch_norm, name=name)\n",
        "\n",
        "    return H\n",
        "\n",
        "def hybrid_unet_base(input_tensor, filter_num, conv, kernel_size=3, stack_num_down=2, stack_num_up=2, \n",
        "                        dilation_rate=1, \n",
        "                        filter_double=False, \n",
        "                        recur_status=(True, True), recur_num=2, \n",
        "                        is_residual=True, \n",
        "                        is_attention=False, \n",
        "                        atten_activation='ReLU', attention='add',\n",
        "                        activation='ReLU', batch_norm=False, pool=True, unpool=True, name='res_unet'):\n",
        "    \n",
        "    '''\n",
        "    The base of Recurrent Residual (R2) U-Net.\n",
        "    \n",
        "    hybrid_unet_base(input_tensor, filter_num, conv, stack_num_down=2, stack_num_up=2, recur_num=2,\n",
        "                    activation='ReLU', batch_norm=False, pool=True, unpool=True, name='res_unet')\n",
        "    \n",
        "    ----------\n",
        "    Alom, M.Z., Hasan, M., Yakopcic, C., Taha, T.M. and Asari, V.K., 2018. Recurrent residual convolutional neural network \n",
        "    based on u-net (r2u-net) for medical image segmentation. arXiv preprint arXiv:1802.06955.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_tensor: the input tensor of the base, e.g., `keras.layers.Inpyt((None, None, 3))`.\n",
        "        filter_num: a list that defines the number of filters for each \\\n",
        "                    down- and upsampling levels. e.g., `[64, 128, 256, 512]`.\n",
        "                    The depth is expected as `len(filter_num)`.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        stack_num_down: number of stacked recurrent convolutional layers per downsampling level/block.\n",
        "        stack_num_down: number of stacked recurrent convolutional layers per upsampling level/block.\n",
        "        recur_num: number of recurrent iterations.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.                 \n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "\n",
        "    X = input_tensor\n",
        "    X_skip = []\n",
        "\n",
        "    # downsampling blocks\n",
        "    X = RR_CONV(X, filter_num[0], conv=conv, stack_num=stack_num_down,\n",
        "                dilation_rate=dilation_rate, filter_double=filter_double,                \n",
        "                recur_status=recur_status, recur_num=recur_num, \n",
        "                is_residual=is_residual,\n",
        "                activation=activation, batch_norm=batch_norm, name='{}_down0'.format(name))\n",
        "    X_skip.append(X)\n",
        "    \n",
        "    for i, f in enumerate(filter_num[1:]):\n",
        "        X = UNET_RR_left(X, f, conv=conv, kernel_size=kernel_size,\n",
        "                          stack_num=stack_num_down, \n",
        "                          dilation_rate=dilation_rate, \n",
        "                          filter_double=filter_double, \n",
        "                          recur_status=recur_status, recur_num=recur_num, \n",
        "                          is_residual=is_residual, \n",
        "                          is_attention=is_attention, \n",
        "                          atten_activation=atten_activation, attention=attention,\n",
        "                          activation=activation, pool=pool, batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))        \n",
        "        X_skip.append(X)\n",
        "    \n",
        "    # upsampling blocks\n",
        "    X_skip = X_skip[:-1][::-1]\n",
        "    for i, f in enumerate(filter_num[:-1][::-1]):\n",
        "        X = UNET_RR_right(X, X_skip[i], f, conv=conv, kernel_size=kernel_size, \n",
        "                          stack_num=stack_num_up, \n",
        "                          dilation_rate=dilation_rate, \n",
        "                          filter_double=filter_double, \n",
        "                          recur_status=recur_status, recur_num=recur_num, \n",
        "                          is_residual=is_residual, \n",
        "                          is_attention=is_attention, \n",
        "                          atten_activation=atten_activation, attention=attention,\n",
        "                          activation=activation, unpool=unpool, batch_norm=batch_norm, name='{}_up{}'.format(name, i+1))\n",
        "    \n",
        "    return X\n",
        "\n",
        "def hybrid_unet(input_size, filter_num, n_labels, conv,\n",
        "               stack_num_down=2, stack_num_up=2, \n",
        "               dilation_rate=1,\n",
        "               filter_double=False,\n",
        "               recur_status=(True, True), recur_num=2,\n",
        "               is_residual=True,\n",
        "               is_attention=True,\n",
        "               atten_activation='ReLU', attention='add',\n",
        "               activation='ReLU', output_activation='Softmax', \n",
        "               batch_norm=False, pool=True, unpool=True, name='hybrid_unet'):\n",
        "\n",
        "    '''\n",
        "    Recurrent Residual (R2) U-Net\n",
        "    \n",
        "    hybrid_unet(input_size, filter_num, n_labels, conv,\n",
        "               stack_num_down=2, stack_num_up=2, recur_num=2,\n",
        "               activation='ReLU', output_activation='Softmax', \n",
        "               batch_norm=False, pool=True, unpool=True, name='hybrid_unet')\n",
        "    \n",
        "    ----------\n",
        "    Alom, M.Z., Hasan, M., Yakopcic, C., Taha, T.M. and Asari, V.K., 2018. Recurrent residual convolutional neural network \n",
        "    based on u-net (r2u-net) for medical image segmentation. arXiv preprint arXiv:1802.06955.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_size: the size/shape of network input, e.g., `(128, 128, 3)`.\n",
        "        filter_num: a list that defines the number of filters for each \\\n",
        "                    down- and upsampling levels. e.g., `[64, 128, 256, 512]`.\n",
        "                    The depth is expected as `len(filter_num)`.\n",
        "        n_labels: number of output labels.\n",
        "        conv: (str) 2d or 3d convolution. e.g. '2d' or '3d'\n",
        "        stack_num_down: number of stacked recurrent convolutional layers per downsampling level/block.\n",
        "        stack_num_down: number of stacked recurrent convolutional layers per upsampling level/block.\n",
        "        recur_num: number of recurrent iterations.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'.\n",
        "        output_activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                           Default option is 'Softmax'.\n",
        "                           if None is received, then linear activation is applied.     \n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.                  \n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras model.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "\n",
        "    IN = Input(input_size, name='{}_input'.format(name))\n",
        "\n",
        "    # base\n",
        "    X = hybrid_unet_base(IN, filter_num, conv=conv, kernel_size=3,\n",
        "                        stack_num_down=stack_num_down, stack_num_up=stack_num_up, \n",
        "                        dilation_rate=dilation_rate, \n",
        "                        filter_double=filter_double, \n",
        "                        recur_status=recur_status, recur_num=recur_num, \n",
        "                        is_residual=is_residual, \n",
        "                        is_attention=is_attention, \n",
        "                        atten_activation=atten_activation, attention=attention,\n",
        "                        activation=activation, batch_norm=batch_norm, pool=pool, unpool=unpool, name=name)\n",
        "    # output layer\n",
        "    OUT = CONV_output(X, conv, n_labels, kernel_size=1, activation=output_activation, name='{}_output'.format(name))\n",
        "    \n",
        "    # functional API model\n",
        "    model = Model(inputs=[IN], outputs=[OUT], name='{}_model'.format(name))\n",
        "    \n",
        "    return model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjUi5V7Z5gJo"
      },
      "source": [
        "### Model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eUbqbKR5fXh"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "IMG_HEIGHT = 512 #img_train.shape[1]\n",
        "IMG_WIDTH  = 512 #img_train.shape[2]\n",
        "IMG_DEPTH  = 512 # for 3D\n",
        "IMG_CHANNELS = 3 #img_train.shape[3]\n",
        "CONV = '2d'\n",
        "NUM_LABELS = 2  #Binary\n",
        "input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS)\n",
        "# input_shape = (IMG_HEIGHT,IMG_WIDTH,IMG_DEPTH, IMG_CHANNELS) # uncomment for 3D\n",
        "batch_size = 2\n",
        "FILTER_NUM = [32, 64, 128, 256, 512]\n",
        "STACK_NUM_DOWN = 2\n",
        "STACK_NUM_UP = 2\n",
        "DILATION_RATE = 1\n",
        "FILTER_DOUBLE = True\n",
        "RECUR_STATUS = (False, True)\n",
        "RECUR_NUM = 2\n",
        "IS_RESIDUAL = True\n",
        "IS_ATTENTION = True\n",
        "ATTENTION_ACTIVATION = 'ReLU'\n",
        "ATTENTION = 'add'\n",
        "ACTIVATION = 'ReLU'\n",
        "OUTPUT_ACTIVATION = 'Softmax'\n",
        "BATCH_NORM = True\n",
        "POOL = False\n",
        "UNPOOL = False\n",
        "RETRAIN = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "mXc0HdP0WR6O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxq4jJTfN1WQ"
      },
      "outputs": [],
      "source": [
        "# Current version works for \"stack_num_down = stack_num_up\" only\n",
        "model = hybrid_unet(input_shape, filter_num=FILTER_NUM, \n",
        "                       n_labels=NUM_LABELS, \n",
        "                       conv = CONV,\n",
        "                       stack_num_down=STACK_NUM_DOWN, stack_num_up=STACK_NUM_UP, \n",
        "                       dilation_rate=DILATION_RATE,\n",
        "                       filter_double=FILTER_DOUBLE,\n",
        "                       recur_status=RECUR_STATUS, recur_num=RECUR_NUM,\n",
        "                       is_residual=IS_RESIDUAL,\n",
        "                       is_attention=IS_ATTENTION,\n",
        "                       atten_activation=ATTENTION_ACTIVATION, attention=ATTENTION,\n",
        "                       activation=ACTIVATION, output_activation=OUTPUT_ACTIVATION, \n",
        "                       batch_norm=BATCH_NORM, pool=POOL, unpool=UNPOOL, name='hybrid_unet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUICq517N1MA"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CNbogwi_6Yi"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A hybrid loss function is implemented consisting of cross-entropy loss and dice coefficient loss."
      ],
      "metadata": {
        "id": "UhEL_vDpVtLF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCF9h10r_5Of"
      },
      "outputs": [],
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "    y_true = tf.image.convert_image_dtype(y_true, tf.float32)\n",
        "    y_pred = tf.image.convert_image_dtype(y_pred, tf.float32)\n",
        "    \n",
        "    loss_ce = keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "    loss_dice_coef = losses.dice_coef(y_true, y_pred)\n",
        "    \n",
        "    return loss_ce + (0.5*loss_dice_coef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZaGz8n_AIMi"
      },
      "source": [
        "### Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2F1Vu299Au0"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=loss_function, optimizer=Adam(learning_rate = 1e-3), \n",
        "              metrics=['accuracy', losses.dice_coef])\n",
        "\n",
        "# print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks and checkpoints"
      ],
      "metadata": {
        "id": "u2cDE7gsVkcO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvyWwBI0Gcte"
      },
      "outputs": [],
      "source": [
        "# Create checkpoint\n",
        "checkpoint_folder = \"recur_F_T_residual_filter_double\"\n",
        "checkpoint_subfolder = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "checkpoint_base_dir = \"/content/drive/MyDrive/panoramicDentalSegmentation/checkpoints/hybrid_unet_2d\"\n",
        "checkpoint_loc = checkpoint_base_dir + '//' + checkpoint_folder + \"//\" + checkpoint_subfolder\n",
        "log_path = \"/content/drive/MyDrive/panoramicDentalSegmentation/logs/\"\n",
        "\n",
        "# Create checkpoint directory if does not exist\n",
        "if not os.path.exists(checkpoint_loc):\n",
        "    os.makedirs(checkpoint_loc)\n",
        "    \n",
        "checkpoint_path = os.path.join(checkpoint_loc, \"cp-{epoch:04d}.ckpt\")\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "callbacks = [\n",
        "    # EarlyStopping(monitor='', patience=400, verbose=1),\n",
        "    ReduceLROnPlateau(factor=0.1,\n",
        "                      monitor='val_loss',\n",
        "                      patience=10,\n",
        "                      min_lr=0.00001,\n",
        "                      verbose=1,\n",
        "                      mode='auto'),\n",
        "    ModelCheckpoint(checkpoint_path,\n",
        "                      monitor = 'val_loss',\n",
        "                      verbose = 1,\n",
        "                      save_best_only=False,\n",
        "                      save_weights_only=False,\n",
        "                      period=5),\n",
        "    CSVLogger(os.path.join(log_path, datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '.csv'), separator=',', append=True),\n",
        "    # TerminateOnNaN()\n",
        "]\n",
        "\n",
        "print(checkpoint_folder)\n",
        "print(checkpoint_subfolder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJgPGqX3AcYc"
      },
      "source": [
        "### Save info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dictionary file to store model information"
      ],
      "metadata": {
        "id": "pCbh9s5cVZh2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMkjzZMpAamy"
      },
      "outputs": [],
      "source": [
        "info_data = {\n",
        "    'network_name': checkpoint_folder,\n",
        "    'checkpoint_subfolder': checkpoint_subfolder, \n",
        "    'loss': \"loss_ce + (0.5*loss_dice_coef)\", \n",
        "    'conv': CONV,\n",
        "    'num_labels': NUM_LABELS,  \n",
        "    'input_shape': (IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),\n",
        "    'batch_size': batch_size,\n",
        "    'filters': FILTER_NUM,\n",
        "    'stack_num_down': STACK_NUM_DOWN,\n",
        "    'stack_num_up': STACK_NUM_UP,\n",
        "    'dilation_rate': DILATION_RATE,\n",
        "    'filter_double': FILTER_DOUBLE,\n",
        "    'recur_status': RECUR_STATUS,\n",
        "    'recur_num': RECUR_NUM,\n",
        "    'is_residual': IS_RESIDUAL,\n",
        "    'is_attention': IS_ATTENTION,\n",
        "    'attention_activation': ATTENTION_ACTIVATION,\n",
        "    'attention': ATTENTION,\n",
        "    'activation': ACTIVATION,\n",
        "    'output_activation': OUTPUT_ACTIVATION,\n",
        "    'batch_norm': BATCH_NORM,\n",
        "    'pool': POOL,\n",
        "    'unpool': UNPOOL,\n",
        "    'retrain': RETRAIN,\n",
        "}\n",
        "\n",
        "# Save in json file\n",
        "json_name = checkpoint_folder + \"_\" + checkpoint_subfolder\n",
        "info_file = open(os.path.join(checkpoint_base_dir, json_name), \"w\")\n",
        "json.dump(info_data, info_file)\n",
        "info_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B53hcr3_dss_"
      },
      "source": [
        "### Data loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It loads data batch-wise from a given directory. <br>\n",
        "\n",
        "Loading the entire training and validation dataset is memory expensive for Colab. Instead, a data loader class called `DataGenerator` is implemented that loads images on-the-fly. In other words, it takes a list of image names and the directory where the images are situated. Then it loads images batch-wise while training the model. <br>\n",
        "\n",
        "Reference: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
      ],
      "metadata": {
        "id": "8d3KqENTOGpr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUO0_Rexdrfg"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self,\n",
        "                 list_IDs,\n",
        "                 dir_image,\n",
        "                 dir_mask,\n",
        "                 batch_size=1,\n",
        "                 dim=(512, 512),\n",
        "                 n_channels_image=3,\n",
        "                 n_channels_mask=1,\n",
        "                 n_classes=2,\n",
        "                 shuffle=True):\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.dir_image = dir_image\n",
        "        self.dir_mask = dir_mask\n",
        "        self.n_channels_image = n_channels_image\n",
        "        self.n_channels_mask = n_channels_mask\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # Counts the number of possible batches that can be made from the total available datasets in list_IDs\n",
        "        # Rule of thumb, num_datasets % batch_size = 0, so every sample is seen\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Gets the indexes of batch_size number of data from list_IDs for one epoch\n",
        "        # If batch_size = 8, 8 files/indexes from list_ID are selected\n",
        "        # Makes sure that on next epoch, the batch does not come from same indexes as the previous batch\n",
        "        # The same batch is not seen again until __len()__ - 1 batches are done\n",
        "\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) *\n",
        "                               self.batch_size]\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "\n",
        "            X = np.empty((self.batch_size, *self.dim, self.n_channels_image)) # 3 for color image\n",
        "            y = np.empty((self.batch_size, *self.dim, self.n_channels_mask)) # 1 for binary/grayscale image\n",
        "\n",
        "            for i, ID in enumerate(list_IDs_temp):\n",
        "                # Write logic for selecting/manipulating X and y here\n",
        "                ID_only = os.path.splitext(ID)[0]\n",
        "                xt = Image.open(os.path.join(self.dir_image, ID_only + '.png'))                \n",
        "                yt = Image.open(os.path.join(self.dir_mask, ID_only + '.png'))\n",
        "\n",
        "                # Do normalization\n",
        "                xt = np.array(xt)/255.0\n",
        "                yt = np.expand_dims(np.array(yt), axis=-1) * 1       # Here, no normalization is needed for the mask image. Because it's a binary image.\n",
        "\n",
        "                X[i,] = xt\n",
        "                y[i,] = yt\n",
        "\n",
        "            y = tensorflow.keras.utils.to_categorical(y, num_classes=self.n_classes, dtype =\"int8\") # num_classes may vary \n",
        "            return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN_OfC52At20"
      },
      "source": [
        "### Return to main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdgbkB-hfDL-"
      },
      "outputs": [],
      "source": [
        "list_IDs_train = os.listdir(img_dir_train)\n",
        "list_IDs_val = os.listdir(img_dir_val)\n",
        "\n",
        "# Uncomment if you want to work with selected numbers of images\n",
        "selected_img = False\n",
        "if selected_img:\n",
        "  import random\n",
        "  random.shuffle(list_IDs_train)\n",
        "  random.shuffle(list_IDs_val)\n",
        "  list_IDs_train = list_IDs_train[:1000]\n",
        "  list_IDs_val = list_IDs_val[:100]\n",
        "  print(list_IDs_train)\n",
        "\n",
        "# Call DataGenerator\n",
        "train_gen = DataGenerator(list_IDs=list_IDs_train,\n",
        "                          dir_image=img_dir_train,\n",
        "                          dir_mask=mask_dir_train,\n",
        "                          n_channels_image=3,\n",
        "                          n_channels_mask=1,\n",
        "                          n_classes=NUM_LABELS,\n",
        "                          dim=(IMG_HEIGHT,IMG_WIDTH),\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n",
        "val_gen = DataGenerator(list_IDs=list_IDs_val,\n",
        "                          dir_image=img_dir_val,\n",
        "                          dir_mask=mask_dir_val,\n",
        "                          n_channels_image=3,\n",
        "                          n_channels_mask=1,\n",
        "                          n_classes=NUM_LABELS,\n",
        "                          dim=(IMG_HEIGHT,IMG_WIDTH),\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFnjyPeNNQ4w"
      },
      "source": [
        "### Run the cell to retrain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell if you want to start training from a given checkpoint. "
      ],
      "metadata": {
        "id": "0j8UAr3NVR1-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTDhgdulln1r"
      },
      "outputs": [],
      "source": [
        "if RETRAIN:\n",
        "  # Load model from checkpoint\n",
        "  from keras.models import load_model\n",
        "  model = load_model('/content/drive/MyDrive/panoramicDentalSegmentation/checkpoints/hybrid_unet_2d/recur_F_T_residual_filter_double/2022-02-24_06-24-34/cp-0025.ckpt', \n",
        "                    compile=False,\n",
        "                    custom_objects={'dice_coef':losses.dice_coef})\n",
        "  model.compile(loss=loss_function, optimizer=Adam(learning_rate = 1e-3), \n",
        "              metrics=['accuracy', losses.dice_coef])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvdIiNw1NXcZ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyfiV-h5n_rJ"
      },
      "outputs": [],
      "source": [
        "hist = model.fit_generator(train_gen,\n",
        "                    steps_per_epoch=len(train_gen),\n",
        "                    validation_data = val_gen,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    callbacks=callbacks,\n",
        "                    shuffle = False,  # already shuffled in data generator \n",
        "                    # workers=20,\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yU1u9XHfFuX"
      },
      "source": [
        "### Loss curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tar_sGMBH0XT"
      },
      "outputs": [],
      "source": [
        "#plot the training and validation IoU and loss at each epoch\n",
        "dir_plot_save = '/content/drive/MyDrive/panoramicDentalSegmentation/'\n",
        "loss = hist.history['loss']\n",
        "val_loss = hist.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(os.path.join(dir_plot_save,'model_loss.png'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW1XM8dfMcdE"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to install jenti. It is required to create (or merge) patches.\n",
        "# !pip install --target='/content/drive/MyDrive/library' jenti"
      ],
      "metadata": {
        "id": "sAffxLO3PHwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMmnu6WmKPqq"
      },
      "outputs": [],
      "source": [
        "from jenti.patch import Patch, Merge\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEfvwdSUBSsR"
      },
      "outputs": [],
      "source": [
        "checkpoint_base_dir = \"/content/drive/MyDrive/panoramicDentalSegmentation/checkpoints/hybrid_unet_2d\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG20atbFCQr8"
      },
      "outputs": [],
      "source": [
        "os.listdir(checkpoint_base_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wJoSbv2Ambk"
      },
      "outputs": [],
      "source": [
        "# Load json file. It contains model info\n",
        "info_file = open(os.path.join(checkpoint_base_dir, json_name), \"r\")\n",
        "output = info_file.read()\n",
        "info_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAIT7GRtLWGs"
      },
      "outputs": [],
      "source": [
        "# Load model from checkpoint\n",
        "from keras.models import load_model\n",
        "# checkpoint_folder = output[\"network_name\"] + \"//\" + output[\"checkpoint_subfolder\"]\n",
        "checkpoint_folder = 'recur_F_T_residual_filter_double' + '//' + '2022-02-24_06-24-34'\n",
        "checkpoint_name = 'cp-0025'\n",
        "model = load_model('/content/drive/MyDrive/panoramicDentalSegmentation/checkpoints/hybrid_unet_2d/' + checkpoint_folder + '//' + checkpoint_name + '.ckpt', \n",
        "                   compile=False,\n",
        "                   custom_objects={'dice_coef':losses.dice_coef})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBhYA2reVvz5"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF9OYhzuKWEv"
      },
      "outputs": [],
      "source": [
        "# Directories \n",
        "load_dir_test_img = '/content/drive/MyDrive/panoramicDentalSegmentation/test_dataset/images'\n",
        "load_dir_test_mask = '/content/drive/MyDrive/panoramicDentalSegmentation/test_dataset/mask'\n",
        "\n",
        "save_dir_pred = '/content/drive/MyDrive/panoramicDentalSegmentation/prediction/hybrid_unet_2d/' + checkpoint_folder + '//' + checkpoint_name\n",
        "if not os.path.exists(save_dir_pred): os.makedirs(save_dir_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrM85XswKtHV"
      },
      "outputs": [],
      "source": [
        "# Test names\n",
        "names_test = os.listdir(load_dir_test_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2gLiCAHK8m6"
      },
      "outputs": [],
      "source": [
        "# Create dataframe to store records\n",
        "df = pd.DataFrame(index=[], columns = [\n",
        "    'Name', 'Accuracy', 'Specificity', 'Precision', 'Recall', 'Dice'], dtype='object')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38SiFZALMwBB"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "patch_shape = [512, 512]\n",
        "overlap = [10, 10]\n",
        "save_pred = True\n",
        "# Iterate over test samples\n",
        "for i, name in enumerate(names_test):\n",
        "  name_only = os.path.splitext(name)[0]\n",
        "  # Load image and mask\n",
        "  im = Image.open(os.path.join(load_dir_test_img, name))\n",
        "  mask = Image.open(os.path.join(load_dir_test_mask, name))\n",
        "  im = np.array(im) # convert to array\n",
        "  im = im/255.0 # normalization\n",
        "  mask = np.array(mask)*1 # convert to array. Multiplied by 1 to covert from boolean to int\n",
        "  mask = mask.astype('int8')\n",
        "  # Create patches from the image\n",
        "  patch = Patch(patch_shape, overlap, patch_name=name_only, csv_output=False)\n",
        "  patches, info, org_shape_im = patch.patch2d(im)\n",
        "  org_shape_mask = (org_shape_im[0], org_shape_im[1], 1) # mask is a binary image\n",
        "  # Iterate over patches\n",
        "  patchwise_pred = [] # store patch-wise predictions for each test sample\n",
        "  for patch in patches:\n",
        "    patch = np.expand_dims(patch, axis=0) # shape: 1 x 512 x 512 x 3\n",
        "    pred2 = model.predict(patch)\n",
        "    # print(pred2.shape)\n",
        "    pred = np.argmax(model.predict(patch), axis=-1) # shape: 1 x 512 x 512\n",
        "    pred = np.expand_dims(np.squeeze(pred), axis=-1) # shape: 512 x 512 x 1\n",
        "    # print(np.max(pred))\n",
        "    patchwise_pred.append(pred)\n",
        "  # Merge patches\n",
        "  merge = Merge(info, org_shape_mask, dtype='int8') # create object\n",
        "  merged = merge.merge2d(patchwise_pred)\n",
        "  # Save prediction as png\n",
        "  if save_pred:\n",
        "    merged_im = Image.fromarray((np.squeeze(merged)*255 ).astype(np.uint8))\n",
        "    merged_im.save(os.path.join(save_dir_pred, 'pred_' + name_only + '.png'))\n",
        "  # Calculate accuracy, specificity, precision, recall, and dice\n",
        "  tn, fp, fn, tp = confusion_matrix(np.squeeze(mask).flatten(), np.squeeze(merged).flatten()).ravel()\n",
        "  acc = ((tp + tn)/(tp + tn + fn + fp))*100  \n",
        "  sp = (tn/(tn + fp))*100\n",
        "  p = (tp/(tp+fp))*100\n",
        "  r = (tp/(tp+fn))*100\n",
        "  # f1 = ((2 * p * r)/(p + r))*100\n",
        "  dice = (2 * tp / (2 * tp + fp + fn))*100\n",
        "  print(\"Img # {:1s}, Image {:1s}: acc: {:3f}, sp: {:3f}, p: {:3f}, r: {:3f}, dice: {:3f}\".format(str(i+1), name_only, acc, sp, p, r, dice))\n",
        "  # Add to dataframe\n",
        "  tmp = pd.Series([name_only, acc, sp, p, r, dice], index=['Name', 'Accuracy', 'Specificity', 'Precision', 'Recall', 'Dice'])\n",
        "  df = df.append(tmp, ignore_index = True)\n",
        "  df.to_csv(os.path.join(save_dir_pred, 'result.csv'), index=False)\n",
        "\n",
        "print(\"Mean Accuracy: \", df[\"Accuracy\"].mean())\n",
        "print(\"Mean Specificity: \", df[\"Specificity\"].mean())\n",
        "print(\"Mean precision: \", df[\"Precision\"].mean())\n",
        "print(\"Mean recall: \", df[\"Recall\"].mean())\n",
        "print(\"Mean Dice: \", df[\"Dice\"].mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRoRb7w6Nyhq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Colab_hybrid_unet_2d_3d.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
